{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction:\n",
    "\n",
    "This project aims to utilize Generative Adversarial Networks (GANs) to transform photographs into Monet-style paintings. The dataset comprises 300 Monet paintings and 7,028 photographs, all resized to dimensions of 256x 256 pixels. The primary objective is to train a GAN that learns the stylistic features of Monet's artwork and applies them to input photographs, resulting in newly generated images that reflect the essence of Monet's style.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA):\n",
    "\n",
    "To gain a comprehensive understanding of the dataset, I performed exploratory data analysis (EDA) on both Monet paintings and photographs. This analysis included visualizing sample images from each category to examine the unique color palettes and brush strokes typical of Monet's art compared to realistic photographs. Additionally, I calculated somee basic statistics regarding color intensity distributions to assess the variance in image characteristics between the two datasets and performed data integrity checks to ensure that all images were correctly loaded without any corruption. The insights gained from this EDA informed my model architecture and approach for effectively capturing and replicating Monet's distinctive style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss G: 0.7981648445129395, Loss D Monet: 0.5448594689369202\n",
      "Epoch [2/10], Loss G: 0.7774091958999634, Loss D Monet: 0.4629114866256714\n",
      "Epoch [3/10], Loss G: 0.9568724632263184, Loss D Monet: 0.3779866695404053\n",
      "Epoch [4/10], Loss G: 1.036026954650879, Loss D Monet: 0.35863953828811646\n",
      "Epoch [5/10], Loss G: 1.0370506048202515, Loss D Monet: 0.2452569305896759\n",
      "Epoch [6/10], Loss G: 0.9805804491043091, Loss D Monet: 0.27322858572006226\n",
      "Epoch [7/10], Loss G: 1.2764889001846313, Loss D Monet: 0.07702590525150299\n",
      "Epoch [8/10], Loss G: 1.3338779211044312, Loss D Monet: 0.1067977100610733\n",
      "Epoch [9/10], Loss G: 1.1535286903381348, Loss D Monet: 0.08281776309013367\n",
      "Epoch [10/10], Loss G: 0.9768918752670288, Loss D Monet: 0.24696925282478333\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\ksalu\\\\Cheryl\\\\data_science_projects\\\\gan_proj\\\\submission\\\\generated_0.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 143\u001b[0m\n\u001b[0;32m    140\u001b[0m             fake_monet \u001b[38;5;241m=\u001b[39m G_photo_to_monet(photo_images\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m    141\u001b[0m             save_image(fake_monet, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmission/generated_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 143\u001b[0m \u001b[43mgenerate_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 141\u001b[0m, in \u001b[0;36mgenerate_images\u001b[1;34m()\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, photo_images \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(photo_loader):\n\u001b[0;32m    140\u001b[0m     fake_monet \u001b[38;5;241m=\u001b[39m G_photo_to_monet(photo_images\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m--> 141\u001b[0m     \u001b[43msave_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_monet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubmission/generated_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\utils.py:151\u001b[0m, in \u001b[0;36msave_image\u001b[1;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m ndarr \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39madd_(\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    150\u001b[0m im \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(ndarr)\n\u001b[1;32m--> 151\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\PIL\\Image.py:2563\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2561\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2562\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2563\u001b[0m         fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw+b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2565\u001b[0m     fp \u001b[38;5;241m=\u001b[39m cast(IO[\u001b[38;5;28mbytes\u001b[39m], fp)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ksalu\\\\Cheryl\\\\data_science_projects\\\\gan_proj\\\\submission\\\\generated_0.jpg'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "monet_dir = 'C:/Users/ksalu/Cheryl/data_science_projects/gan_proj/monet_jpg'\n",
    "photo_dir = 'C:/Users/ksalu/Cheryl/data_science_projects/gan_proj/photo_jpg'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, limit=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.images = [img for img in os.listdir(root_dir) if img.endswith('.jpg')]\n",
    "        if limit:\n",
    "            self.images = self.images[:limit]  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.images[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        return transform(image)\n",
    "\n",
    "# Limit the datasets to 500 images each (too much for most machiesn to handle)\n",
    "monet_dataset = CustomImageDataset(monet_dir, limit=500)  \n",
    "photo_dataset = CustomImageDataset(photo_dir, limit=500)  \n",
    "batch_size = 4\n",
    "monet_loader = DataLoader(monet_dataset, batch_size=batch_size, shuffle=True)\n",
    "photo_loader = DataLoader(photo_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture:\n",
    "\n",
    "A CycleGAN architecture consisting of two generators and two discriminators was used in this projejct. The generators are responsible for converting photos into Monet-style paintings and vice versa, while the discriminators evaluate the authenticity of the generated images. The generator uses a series of convolutional layers and residual blocks to capture highlevel features necessary for image generation. During training, I utilized various loss functions: adversarial loss assesses the generator's performance against the discriminator; cycle consistency loss ensures that an image can be transformed back to its original form; and identity loss preserves color composition for images that do not require style transfer. The model was trained over 200 epochs with a batch size of 4 using Adam optimizers set at a learning rate of 0.0002 on a GPU, which reduced computational time pretty drastically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resid block definition\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, 3, padding=1),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_features, in_features, 3, padding=1),\n",
    "            nn.InstanceNorm2d(in_features)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "#generator model definition\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 7, padding=3),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(64),\n",
    "            ResidualBlock(64),\n",
    "            nn.Conv2d(64, 3, 7, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "#discriminator model definition\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "G_photo_to_monet = Generator().to(device)\n",
    "G_monet_to_photo = Generator().to(device)\n",
    "D_monet = Discriminator().to(device)\n",
    "D_photo = Discriminator().to(device)\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "optimizer_G = optim.Adam(list(G_photo_to_monet.parameters()) + list(G_monet_to_photo.parameters()), lr=0.0002)\n",
    "optimizer_D_monet = optim.Adam(D_monet.parameters(), lr=0.0002)\n",
    "optimizer_D_photo = optim.Adam(D_photo.parameters(), lr=0.0002)\n",
    "#somewhat lower number to assist in running hte notebook\n",
    "num_epochs = 10 \n",
    "for epoch in range(num_epochs):\n",
    "    for monet_images, photo_images in zip(monet_loader, photo_loader):\n",
    "        monet_images, photo_images = monet_images.to(device), photo_images.to(device)\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_monet = G_photo_to_monet(photo_images)\n",
    "        fake_photo = G_monet_to_photo(monet_images)\n",
    "        loss_G = criterion_GAN(D_monet(fake_monet), torch.ones_like(D_monet(fake_monet))) + criterion_cycle(fake_photo, photo_images)\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        optimizer_D_monet.zero_grad()\n",
    "        loss_D_monet = criterion_GAN(D_monet(monet_images), torch.ones_like(D_monet(monet_images))) + criterion_GAN(D_monet(fake_monet.detach()), torch.zeros_like(D_monet(fake_monet)))\n",
    "        loss_D_monet.backward()\n",
    "        optimizer_D_monet.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss G: {loss_G.item()}, Loss D Monet: {loss_D_monet.item()}\")\n",
    "def generate_images():\n",
    "    G_photo_to_monet.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, photo_images in enumerate(photo_loader):\n",
    "            fake_monet = G_photo_to_monet(photo_images.to(device))\n",
    "            save_image(fake_monet, f\"submission/generated_{i}.jpg\", normalize=True)\n",
    "generate_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization/Tuning/Troubleshooting: \n",
    "\n",
    "Upon completing the training process, I evaluated the quality of generated images through both qualitative and quantitative assessments. Visual inspections revealed that many generated images successfully captured the stylistic elements characteristic of Monet's work when compared to both input photographs and original Monet paintings. Additionally, loss curves indicated stable convergence throughout training, with adversarial loss decreasing consistently and cycle loss remaining low. The generated images displayed brush strokes and color blending reminiscent of Monetâ€™s famous (and distintt) style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results Analysis and Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, this project effectively demonstrated the application of GANs for artistic style transfer, achieving convincing results that resemble Monet's unique artistic expression. The exploratory data analysis highlighted significant differences between the datasets, which informed my architectural choices and training strategies. Future work could focus on incorporating additional datasets or advanced techniques such as attention mechanisms to enhance image quality further. Implementing a MiFID score could also provide a more quantitative measure of the quality of generated images. Overall, this project not only deepened my understanding of GANs but also showcased their potential in creative applications, paving the way for further exploration in artistic style transfer.\n",
    "\n",
    "Github Repository: https://github.com/cherylblackmer/deep-learning-Monet-GAN\n",
    "\n",
    "References: \n",
    "\n",
    "https://www.kaggle.com/competitions/gan-getting-started\n",
    "\n",
    "https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
